{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данный скрипт ежедневно скрейпит топ 100 Сберзвука\n",
    "\n",
    "# время запуска: 18:25 МСК\n",
    "# ВАЖНО: записываемая дата = день скрейпинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDT = datetime.now() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://sber-zvuk.com/top100'\n",
    "r = requests.get(base_url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "s = str(soup)\n",
    "songs = [i.strip('\"') for i in re.findall(r'\"title\":\\s*(.*?)\\s*\\,', s)[1:101] ]\n",
    "songs = [\" \".join(i.split(\"\\xa0\")) for i in songs]\n",
    "songs = [\" & \".join(i.split(\" \\\\u0026 \")) for i in songs]\n",
    "\n",
    "artists = [i.strip('\"') for i in re.findall(r'(?<=\"credits\":\")(.*?)(?=\",)', s)[:100] ]\n",
    "artists = [\" & \".join(i.split(\" \\\\u0026 \")) for i in artists]\n",
    "\n",
    "\n",
    "data = {\"rank\": [i for i in range(1, len(songs)+1)], \"title\": songs, \"artist\":artists}\n",
    "sber = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = currentDT \n",
    "sber[\"date\"] = datetime.strftime(date,\"%d/%m/%Y\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# берем имеющийся csv файл и обновляем его\n",
    "if path.exists(\"all_sber.csv\") == True:\n",
    "    all_sber = pd.read_csv(\"all_sber.csv\")\n",
    "    all_sber = all_sber.drop(all_sber.columns[[0]], axis=1) # удаляем получающуюся после импорта лишнюю колонку \n",
    "    \n",
    "    # чистим дубликаты (опыт показал, что они бывают)\n",
    "    all_sber.drop_duplicates(inplace= True)\n",
    "    all_sber.reset_index(inplace=True)\n",
    "    all_sber.drop(all_sber.columns[[0]], axis=1, inplace=True)\n",
    "    \n",
    "    if datetime.strftime(date, \"%d/%m/%Y\") in set(all_sber[\"date\"]):\n",
    "        print(date, \": this date's SBER chart is already in the data. I expect the new script to be superior so I am overwriting the old data.\")\n",
    "        all_sber = all_sber[all_sber[\"date\"]!=datetime.strftime(date,\"%d/%m/%Y\")]\n",
    "    else:\n",
    "        print(date, \": this date's SBER chart is not in our data yet. I proceed to save it.\")\n",
    "    \n",
    "    frames = [all_sber, sber]\n",
    "    all_sber = pd.concat(frames, sort=False)\n",
    "    all_sber.to_csv(\"all_sber.csv\", encoding = \"utf-8\")\n",
    "else:\n",
    "    sber.to_csv(\"all_sber.csv\", encoding = \"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
