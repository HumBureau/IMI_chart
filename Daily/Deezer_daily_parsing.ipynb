{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-08 14:02:13.771911 : New Deezer chart is found. No more scraping for today!\n"
     ]
    }
   ],
   "source": [
    "# грузим данные за предыдущие дни\n",
    "all_deezer = pd.read_csv(\"all_deezer.csv\")\n",
    "all_deezer = all_deezer.drop(all_deezer.columns[[0]], axis=1) # удаляем получающуюся после импорта лишнюю колонку \n",
    "\n",
    "# на всякий случай чистим от дублей\n",
    "all_deezer = all_deezer.drop_duplicates()\n",
    "all_deezer.reset_index(inplace=True) \n",
    "all_deezer.drop(all_deezer.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "# берем последние 100 строк, чтобы сверить с новыми\n",
    "old_df=all_deezer[-100:]\n",
    "o_l = list(old_df[\"title\"])\n",
    "    \n",
    "    \n",
    "# базовая ссылка на последний актуальный ежедневный чарт по России\n",
    "request_deezer = requests.get('https://api.deezer.com/playlist/1116189381') # ссылка на постоянный плейлист\n",
    "deezer_chart_json = request_deezer.json() # через API получаем json \n",
    "new_df = pd.DataFrame(deezer_chart_json['tracks']['data']) # выбираем только список треков\n",
    "\n",
    "# Находим имена ВСЕХ артистов для каждого трека через API трека\n",
    "\n",
    "A_l = []\n",
    "for i in new_df[\"id\"]:\n",
    "    api_track = 'https://api.deezer.com/track/'+str(i)\n",
    "    request_deezer = requests.get(api_track) \n",
    "    json = request_deezer.json()\n",
    "    a_l = []\n",
    "    for j in json[\"contributors\"]:\n",
    "        a_l.append(j[\"name\"])\n",
    "    g_a_l = [i for n, i  in enumerate(a_l) if i not in a_l[:n]] \n",
    "    artists = \", \".join(g_a_l) #  delete duplicate mentions\n",
    "    A_l.append(artists)\n",
    "new_df[\"artist\"] = A_l\n",
    "\n",
    "new_df['rank'] = new_df.reset_index().index +1 \n",
    "new_df = new_df[['rank', 'title', 'artist']]\n",
    "    \n",
    "n_l = list(new_df[\"title\"])   \n",
    "        \n",
    "# новый ли тот чат, который мы заскрейпили?\n",
    "if o_l != n_l:           \n",
    "            \n",
    "    # задаем дату\n",
    "    date = datetime.now() \n",
    "    new_df[\"date\"] = datetime.strftime(date,\"%d/%m/%Y\")  \n",
    "            \n",
    "    # вписываем данные в csv\n",
    "    frames = [all_deezer, new_df]\n",
    "    all_deezer = pd.concat(frames, sort=False)\n",
    "    all_deezer.reset_index(inplace=True) \n",
    "    all_deezer.drop(all_deezer.columns[[0]], axis=1, inplace=True)\n",
    "    all_deezer.to_csv(\"all_deezer.csv\", encoding = \"utf-8\")\n",
    "    \n",
    "    print(date, \": New Deezer chart is found. No more scraping for today!\")\n",
    "            \n",
    "else:\n",
    "    all_deezer.to_csv(\"all_deezer.csv\", encoding = \"utf-8\") # сохраняем на всякий случай, если вдруг были дубли и мы их почистили\n",
    "    print(datetime.now(), \": Keep scraping. No chart found yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
