{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данный скрипт: \n",
    "\n",
    "\n",
    "## - осуществляет парсинг ежедневных чартов\n",
    "### - через selenium: VK\n",
    "\n",
    "\n",
    "## - должен запускаться каждый день один раз в сутки. Самое раннее - в 03:45 утра.\n",
    "## Справка: время обновления исходных чартов.\n",
    "\n",
    "### VK: 2:45 a.m Москва\n",
    "\n",
    "\n",
    "\n",
    "## - на выходе:\n",
    "### - обновляет all_vk.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# задаем команду для получения даты\n",
    "currentDT = datetime.now() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium-часть\n",
    "options = Options()\n",
    "options.add_argument('-headless')\n",
    "br = webdriver.Firefox(executable_path=GeckoDriverManager().install(), options = options)\n",
    "url='https://vk.com'\n",
    "br.get(url)\n",
    "for cookie in pickle.load(open(\"vkcooks.pkl\", \"rb\")): \n",
    "    br.add_cookie(cookie) \n",
    "br.get(url)\n",
    "\n",
    "if br.current_url == \"https://vk.com/feed\":\n",
    "    print(\"great, cookies worked for no-login authorisation\")\n",
    "    # now we proceed with scraping\n",
    "    button2 = br.find_element_by_xpath('//*[@id=\"l_aud\"]/a')\n",
    "    button2.click()\n",
    "    sleep(randint(4,5))\n",
    "    button3 = br.find_element_by_css_selector('div#content li._audio_section_tab__explore > a')\n",
    "    button3.click()\n",
    "    sleep(randint(4,5))\n",
    "    button4 = br.find_element_by_css_selector('div#content div.CatalogBlock__recoms_top_audios_global_header.CatalogBlock__header > div > a')\n",
    "    button4.click()\n",
    "    sleep(randint(10,11))\n",
    "    soup = BeautifulSoup(br.page_source, features=\"lxml\")\n",
    "    br.quit()\n",
    "else:\n",
    "    print(\"ERROR: please do manual login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# работаем с html\n",
    "\n",
    "songs = soup.findAll('span', attrs={'class':\"audio_row__title_inner _audio_row__title_inner\"})\n",
    "artists = soup.findAll('div', attrs={'class':\"audio_row__performers\"})\n",
    "\n",
    "songs_clean = [i.get_text() for i in songs]\n",
    "artists_clean = [i.get_text() for i in artists]\n",
    "\n",
    "data = {\"rank\": [i for i in range(1, 101)], \"title\": songs_clean, \"artist\":artists_clean}\n",
    "vk_music_top_100_daily = pd.DataFrame(data)\n",
    "# дата = предыдущий день (относительно дня скрейпинга)\n",
    "date = currentDT - relativedelta(days=+1)\n",
    "vk_music_top_100_daily[\"date\"] = datetime.strftime(date,\"%d/%m/%Y\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# берем имеющийся csv файл и обновляем его\n",
    "\n",
    "all_vk = pd.read_csv(\"all_vk.csv\")\n",
    "all_vk = all_vk.drop(all_vk.columns[[0]], axis=1) # удаляем получающуюся после импорта лишнюю колонку \n",
    "\n",
    "# чистим дубликаты (опыт показал, что они бывают)\n",
    "all_vk.drop_duplicates(inplace= True)\n",
    "all_vk.reset_index(inplace=True)\n",
    "all_vk.drop(all_vk.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "# проверяем, не сохраняли ли мы уже данные за этот день:\n",
    "if datetime.strftime(date, \"%d/%m/%Y\") in set(all_vk[\"date\"]):\n",
    "    print(now, \": this date's VK data is already saved. Not saving new data.\")\n",
    "else:\n",
    "    print(now, \": this date's VK chart is not in our data yet. I proceed to save it and export to csv.\")\n",
    "    frames = [all_vk, vk_music_top_100_daily]\n",
    "    all_vk = pd.concat(frames, sort=False)\n",
    "    all_vk.to_csv(\"all_vk.csv\", encoding = \"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
